{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculate Gradient"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"x = torch.randn(3, requires_grad=True)\nprint(x)\n\ny = x+2\nprint(y)\n\nz = y*y*2\nprint(z)\n\nz = z.mean()\nprint(z)\n\n# Calculate the gradient\nz.backward() # dz/dx\nprint(x.grad)","execution_count":9,"outputs":[{"output_type":"stream","text":"tensor([1.3924, 0.2586, 0.8945], requires_grad=True)\ntensor([3.3924, 2.2586, 2.8945], grad_fn=<AddBackward0>)\ntensor([23.0167, 10.2027, 16.7561], grad_fn=<MulBackward0>)\ntensor(16.6585, grad_fn=<MeanBackward0>)\ntensor([4.5232, 3.0115, 3.8593])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## If the input are not scalar"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.randn(3, requires_grad=True)\nprint(x)\n\ny = x+2\nprint(y)\n\nz = y*y*2\nprint(z)\n\n# Because backward is Vector Jacobian Product, you need a vector ! \nv = torch.tensor([0.1,1.0,0.001], dtype = torch.float32)\n\n# Calculate the gradient\nz.backward(v) # dz/dx\nprint(x.grad)","execution_count":21,"outputs":[{"output_type":"stream","text":"tensor([-0.4339, -0.5421,  0.3699], requires_grad=True)\ntensor([1.5661, 1.4579, 2.3699], grad_fn=<AddBackward0>)\ntensor([ 4.9052,  4.2510, 11.2332], grad_fn=<MulBackward0>)\ntensor([0.6264, 5.8317, 0.0095])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 3 Ways to prevent for tracking the gradients\n\n```python\n1. x.requires_grad_(False) # Inplacing the gradient to false\n2. x.detach() # Create new tensor that doesn't required the gradient\n3. with torch.no_grad(): \n    pass\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x)\n\n# Prevent the gradient to calculate\nv = x.detach()\nb = x.requires_grad_(False)\n# x.requires_grad_(False)\n\nprint(b)\nprint(v)\n\n# Prevent the gradient to calculate\nwith torch.no_grad():\n    g = y+2\n    print(g)\n\n# Still calculate the gradient\ng = y+2 \nprint(g)","execution_count":22,"outputs":[{"output_type":"stream","text":"tensor([-0.4339, -0.5421,  0.3699], requires_grad=True)\ntensor([-0.4339, -0.5421,  0.3699])\ntensor([-0.4339, -0.5421,  0.3699])\ntensor([3.5661, 3.4579, 4.3699])\ntensor([3.5661, 3.4579, 4.3699], grad_fn=<AddBackward0>)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## grad in looping"},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = torch.ones(4, requires_grad=True)\n\nfor epoch in range(3):\n    model_output = (weights*3).sum()\n    \n    model_output.backward()\n    print(weights.grad)\n    \n    #weights.grad.zero_() # You need to re-assgin to zero\n    #print(weights.grad)","execution_count":25,"outputs":[{"output_type":"stream","text":"tensor([3., 3., 3., 3.])\ntensor([6., 6., 6., 6.])\ntensor([9., 9., 9., 9.])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Backpropagation"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.tensor(1.0)\ny = torch.tensor(2.0)\n\nw = torch.tensor(1.0, requires_grad=True)\n\n# forward pass and compute the loss\ny_hat = w * x\nloss = (y_hat-y)**2\n\nprint(loss)\n\n# backward pass\nloss.backward()\nprint(w.grad)\n\n## update weights\n## next forwards and backwards","execution_count":31,"outputs":[{"output_type":"stream","text":"tensor(1., grad_fn=<PowBackward0>)\ntensor(-2.)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}