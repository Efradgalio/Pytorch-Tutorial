{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create model manually : Linear Regression"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# f = w*x --> f = 2*x\nX = np.array([1,2,3,4],dtype=np.float32)\ny = np.array([2,4,6,8], dtype=np.float32)\n\nw = 0.0\n\n# model prediction\ndef forward(x):\n    return w*x\n\n# loss = MSE\ndef loss(y,y_predicted):\n    return ((y_predicted-y)**2).mean()\n\n\n# gradient\n# MSE = 1/N * (w*x - y)**2\n# dJ/dw = 1/N 2x (w*x - y)\ndef gradient(x,y,y_predicted):\n    return np.dot(2*x, (y_predicted-y)).mean()\n\n\nprint(f'Prediction before training: f(5) = {forward(5):.3f}')\n\n# Training\nlearning_rate = 0.01\nn_iters = 10\n\nfor epoch in range(n_iters):\n    # prediction = forward pass\n    y_pred = forward(X)\n    \n    # loss\n    l = loss(y,y_pred)\n    \n    # gradients\n    dw = gradient(X,y,y_pred)\n    \n    # update weights\n    w -= learning_rate * dw\n    \n    if epoch % 1==0:\n        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n        \n\n\nprint(f'Prediction after training: f(5) = {forward(5):.3f}')","execution_count":6,"outputs":[{"output_type":"stream","text":"Prediction before training: f(5) = 0.000\nepoch 1: w = 1.200, loss = 30.00000000\nepoch 2: w = 1.680, loss = 4.79999924\nepoch 3: w = 1.872, loss = 0.76800019\nepoch 4: w = 1.949, loss = 0.12288000\nepoch 5: w = 1.980, loss = 0.01966083\nepoch 6: w = 1.992, loss = 0.00314570\nepoch 7: w = 1.997, loss = 0.00050332\nepoch 8: w = 1.999, loss = 0.00008053\nepoch 9: w = 1.999, loss = 0.00001288\nepoch 10: w = 2.000, loss = 0.00000206\nPrediction after training: f(5) = 9.999\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression manually but with Autograd"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n# f = w*x --> f = 2*x\nX = torch.tensor([1,2,3,4],dtype=torch.float32)\ny = torch.tensor([2,4,6,8], dtype=torch.float32)\n\nw = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n\n# model prediction\ndef forward(x):\n    return w*x\n\n# loss = MSE\ndef loss(y,y_predicted):\n    return ((y_predicted-y)**2).mean()\n\n\nprint(f'Prediction before training: f(5) = {forward(5):.3f}')\n\n# Training\nlearning_rate = 0.01\nn_iters = 10\n\nfor epoch in range(n_iters):\n    # prediction = forward pass\n    y_pred = forward(X)\n    \n    # loss\n    l = loss(y,y_pred)\n    \n    # gradients = backward pass\n    l.backward() # dl/dw\n    \n    # update weights\n    with torch.no_grad():\n        w -= learning_rate * w.grad\n    \n    # zero gradients\n    w.grad.zero_()\n    if epoch % 1==0:\n        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n        \n\n\nprint(f'Prediction after training: f(5) = {forward(5):.3f}')","execution_count":8,"outputs":[{"output_type":"stream","text":"Prediction before training: f(5) = 0.000\nepoch 1: w = 0.300, loss = 30.00000000\nepoch 2: w = 0.555, loss = 21.67499924\nepoch 3: w = 0.772, loss = 15.66018772\nepoch 4: w = 0.956, loss = 11.31448650\nepoch 5: w = 1.113, loss = 8.17471695\nepoch 6: w = 1.246, loss = 5.90623236\nepoch 7: w = 1.359, loss = 4.26725292\nepoch 8: w = 1.455, loss = 3.08308983\nepoch 9: w = 1.537, loss = 2.22753215\nepoch 10: w = 1.606, loss = 1.60939169\nPrediction after training: f(5) = 8.031\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression model,loss, and optimization automatic\n1. Design model (input, output size, forward pass)\n2. Construct loss and optimizer\n3. Training loop\n    - forward pass: compute prediction\n    - backward pass: gradients\n    - update weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Automatic loss and optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n# f = w*x --> f = 2*x\nX = torch.tensor([1,2,3,4],dtype=torch.float32)\ny = torch.tensor([2,4,6,8], dtype=torch.float32)\n\nw = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n\n# model prediction\ndef forward(x):\n    return w*x\n\nprint(f'Prediction before training: f(5) = {forward(5):.3f}')\n\n# Training\nlearning_rate = 0.01\nn_iters = 10\n\nloss = nn.MSELoss()\noptimizer = torch.optim.SGD([w], lr = learning_rate)\n\nfor epoch in range(n_iters):\n    # prediction = forward pass\n    y_pred = forward(X)\n    \n    # loss\n    l = loss(y,y_pred)\n    \n    # gradients = backward pass\n    l.backward() # dl/dw\n    \n    # update weights\n    optimizer.step()\n    \n    # zero gradients\n    optimizer.zero_grad()\n    \n    if epoch % 1==0:\n        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n        \n\n\nprint(f'Prediction after training: f(5) = {forward(5):.3f}')","execution_count":10,"outputs":[{"output_type":"stream","text":"Prediction before training: f(5) = 0.000\nepoch 1: w = 0.300, loss = 30.00000000\nepoch 2: w = 0.555, loss = 21.67499924\nepoch 3: w = 0.772, loss = 15.66018772\nepoch 4: w = 0.956, loss = 11.31448650\nepoch 5: w = 1.113, loss = 8.17471695\nepoch 6: w = 1.246, loss = 5.90623236\nepoch 7: w = 1.359, loss = 4.26725292\nepoch 8: w = 1.455, loss = 3.08308983\nepoch 9: w = 1.537, loss = 2.22753215\nepoch 10: w = 1.606, loss = 1.60939169\nPrediction after training: f(5) = 8.031\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### automatic model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n# f = w*x --> f = 2*x\n\n# Input size have a certain size\nX = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\ny = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n\nX_test = torch.tensor([5], dtype= torch.float32)\nn_samples, n_features = X.shape\nprint(n_samples, n_features)\n\ninput_size = n_features\noutput_size = n_features\n\nmodel = nn.Linear(input_size, output_size)\n\nprint(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n\n# Training\nlearning_rate = 0.01\nn_iters = 10\n\nloss = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n\nfor epoch in range(n_iters):\n    # prediction = forward pass\n    y_pred = model(X)\n    \n    # loss\n    l = loss(y,y_pred)\n    \n    # gradients = backward pass\n    l.backward() # dl/dw\n    \n    # update weights\n    optimizer.step()\n    \n    # zero gradients\n    optimizer.zero_grad()\n    \n    if epoch % 1==0:\n        [w,b] = model.parameters()\n        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n        \n\n\nprint(f'Prediction after training: f(5) = {model(X_test).item():.3f}')","execution_count":12,"outputs":[{"output_type":"stream","text":"4 1\nPrediction before training: f(5) = 3.533\nepoch 1: w = 0.732, loss = 10.74367905\nepoch 2: w = 0.881, loss = 7.51848364\nepoch 3: w = 1.006, loss = 5.28020954\nepoch 4: w = 1.109, loss = 3.72674084\nepoch 5: w = 1.196, loss = 2.64844537\nepoch 6: w = 1.268, loss = 1.89986444\nepoch 7: w = 1.329, loss = 1.38006854\nepoch 8: w = 1.379, loss = 1.01902401\nepoch 9: w = 1.422, loss = 0.76813543\nepoch 10: w = 1.457, loss = 0.59368360\nPrediction after training: f(5) = 8.320\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### custom model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n# f = w*x --> f = 2*x\n\n# Input size have a certain size\nX = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\ny = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n\nX_test = torch.tensor([5], dtype= torch.float32)\nn_samples, n_features = X.shape\nprint(n_samples, n_features)\n\ninput_size = n_features\noutput_size = n_features\n\nmodel = nn.Linear(input_size, output_size)\n\nclass LinearRegression(nn.Module):\n    \n    def __init__(self,input_dim,output_dim):\n        super(LinearRegression,self).__init__()\n        \n        self.lin = nn.Linear(input_dim,output_dim)\n    def forward(self,x):\n        return self.lin(x)\n\nmodel = LinearRegression(input_size, output_size)\n\nprint(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n\n# Training\nlearning_rate = 0.01\nn_iters = 10\n\nloss = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n\nfor epoch in range(n_iters):\n    # prediction = forward pass\n    y_pred = model(X)\n    \n    # loss\n    l = loss(y,y_pred)\n    \n    # gradients = backward pass\n    l.backward() # dl/dw\n    \n    # update weights\n    optimizer.step()\n    \n    # zero gradients\n    optimizer.zero_grad()\n    \n    if epoch % 1==0:\n        [w,b] = model.parameters()\n        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n        \n\n\nprint(f'Prediction after training: f(5) = {model(X_test).item():.3f}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}